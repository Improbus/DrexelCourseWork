#!/usr/bin/python

import sys

######   LEXER   ###############################
from ply import lex

tokens = (
	'ERROR',
	'CLOSURE',
	'LPAREN',
	'RPAREN',
	'EOL',
	'UNION',
	'CONCAT',
	'LETTER'
)

	# t_ignore is special, and does just what it says
t_ignore = ' \t'

	# These are the simple maps
		# These are all regular expressions.  raw string doesn't translate the \
t_CLOSURE		= r'\*'
t_LPAREN	= r'\('
t_RPAREN	= r'\)'
t_UNION		= r'\|'

def t_LETTER( t ) :
	r'[a-z]'
	t.value = str( t.value )
	return t

def t_CONCAT( t ) :
	r'[a-z]+[a-z]+'
	t.value = str( t.value )
	return t

def t_newline( t ):
  r'\n+'
  t.lexer.lineno += len( t.value )

  # Error handling rule
def t_error( t ):
  print "Illegal character '%s' on line %d" % ( t.value[0], t.lexer.lineno )
  #return t
  t.lexer.skip( 1 )

  # Here is where we build the lexer, after defining it (above)
lex.lex()

######   LEXER (end)   ###############################


def main( arg=sys.argv ) :

		# Now, this lexer actually takes a string; it doesn't (that I yet know)
		# read from a file.  So, you can parse the file as you like, and feed it
		# to the lexer.
	
	# we're going to read a line at a time from stdin

	line_cnt = 0
	
	for line in sys.stdin :

		lex.input( line )
				
		line_cnt += 1
		print "\nLine #", line_cnt
		out = open(str(line_cnt) + '.dot', "w")
		out.write('digraph fsm {\n')
		out.write('rankdir="LR"\n')
		out.write('start [shape="plaintext",label="start"]\n')
		out.write('0 [shape="circle",label="S0"]\n')
		out.write('1 [shape="circle",label="S0"]\n')
		out.write('2 [shape="circle",label="S0"]\n')
		out.write('3 [shape="doublecircle",label="S0"]\n')
		out.write('start->0\n')

		
			# attempt to get that first token
			
		tok = lex.token()
		while tok :
			print tok
			S = str(tok)
			S = S.replace('(', " " )
			S = S.replace(')', "")
			S = S.replace("'" , "")
			S = S.replace("LexToken " , "")
			S = S.split(',')
			if S[0] == 'LETTER' and S[1] == 'a' :
				out.write('0->1 [label="a"]\n')
			elif S[0] == 'LETTER' and S[1] == 'b' :
				out.write('1->0 [label="b"]\n')
			elif S[0] == 'CLOSURE' and S[1] == 'a' :
				out.write('0->1 [label="e"]\n')
				out.write('1->2 [label="a"]\n')
				out.write('2->1 [label="e"]\n')
				out.write('2->3 [label="e"]\n')
				out.write('0->3 [label="e"]\n')
			elif S[0] == 'CLOSURE' and S[1] == 'b' :
				out.write('0->1 [label="e"]\n')
				out.write('1->2 [label="a"]\n')
				out.write('2->1 [label="e"]\n')
				out.write('2->3 [label="e"]\n')
				out.write('0->3 [label="e"]\n')
			elif S[0] == 'CONCAT' and S[1] == 'a' :
				out.write('0->1 [label="a"]\n')
				out.write('1->2 [label="b"]\n')
			elif S[0] == 'CONCAT' and S[1] == 'b' :
				out.write('0->1 [label="a"]\n')
				out.write('1->2 [label="b"]\n')
			elif S[0] == 'UNION' :
				out.write('0->1 [label="a"]\n')
				out.write('0->1 [label="b"]\n')
		
			tok = lex.token()
			
	out.write('}')
		# NOTE:  tok is an instance of LexToken.  Has these attributes:
		#   type - the type, from the 'tokens' list, assigned by magic
		#   value - the string that matched, unless you did something
		#   lineno - the line # (see t_newline())
		#   lexpos - the position of the character, from the beginning


if __name__ == '__main__' :
	main()

